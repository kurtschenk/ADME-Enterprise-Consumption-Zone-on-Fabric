{"cells":[{"cell_type":"markdown","source":["# Setup code for main data table, logging table and run info table\n","\n","\n","Below is code to create the tables needed to export data from ADME/OSDU to Fabric. Use the variables \"table_name\", \"logging_table\" and \"run_info_table\" to give names to the tables. Run the setup code below first, the cell with the schemas, and then the cell that creates the table you want. Notice the setting \"delete_existing_table\". It is default set to \"no\" which means the code will do nothing if the table exists. If this is changed to yes existing table with the same name will be deleted and emptied.\n","\n","## Setup variables for ADME\n","server - They URL to the ADME server. You can find this in the portal on the overview page of the ADME instance\n","\n","Api URLs - These you do not have to change and can use the values already there.\n","\n","data_partition - The name of the data partition you want to export from. Data partitions can be found in the Portal on the ADME instance. Find data partitions on the left side when you have opened the ADME instance.\n","\n","legal_tag - This is the default value that will be put on the exported document if the original does not have a legal_tag\n","\n","acl_viewer - This is the default value that will be put on the exported document if the original does not have a value\n","\n","acl_owner - This is the default value that will be put on the exported document if the original does not have a value\n","\n","authentication_mode - In this example we use \"msal_interactive\" See (insert link here) for more information\n","\n","authority - \"https://login.microsoftonline.com/xxxxx\" where xxxx is your tenantid\n","\n","scopes - [\"xxxx/.default\"], where xxxx is your client_id. NOTE this variable is a list and therefore it needs the square brackets even if it is only one value\n","\n","client_id - this is the app id that was used to create the ADME instance\n","\n","tenant_id - the tenant id. Search Tenant properties in Portal to find this value. This is the tenant where ADME resides.\n","\n","redirect_uri - this value is set on the app used when creating the ADME instance. \n","\n","access_token_type - \"keyvault\", it is strongly recommended that you use a keyvault for the key to access AMDE\n","\n","key_vault_name - the name of the key vault\n","\n","secret_name - the name of the secret in the key vault\n","\n","table_name - the name of the table you store data to in Fabric - put code at bottom of notebook\n","\n","logging_table - the name of the table where logs will be stored\n","\n","run_info_table - the name of the table where last run info is stored. This is used for delta loads since last run time\n","\n","lakehouse_name - the name of the lakehouse where the delta tables will reside\n","\n","## How to use the variables\n","\n","**The variables are called with config[\"variable\"], for example: config[\"server\"]**\n"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"88b079b6-2096-472b-9542-747a096d76af"},{"cell_type":"code","source":["import pandas as pd\n","import json\n","\n","# TODO: Put sensitive information in Key Vault \n","\n","# Correct the JSON string format and load it\n","config_json = '''\n","{\n","    \"server\": \"https://mahuja.energy.azure.com\",\n","    \"crs_catalog_url\": \"/api/crs/catalog/v2/\",\n","    \"crs_converter_url\": \"/api/crs/converter/v2/\",\n","    \"entitlements_url\": \"/api/entitlements/v2/\",\n","    \"file_url\": \"/api/file/v2/\",\n","    \"legal_url\": \"/api/legal/v1/\",\n","    \"schema_url\": \"/api/schema-service/v1/\",\n","    \"search_url\": \"/api/search/v2/\",\n","    \"storage_url\": \"/api/storage/v2/\",\n","    \"unit_url\": \"/api/unit/v3/\",\n","    \"workflow_url\": \"/api/workflow/v1/\",\n","    \"data_partition_id\": \"testdata\",\n","    \"legal_tag\": \"legal_tag\",\n","    \"acl_viewer\": \"acl_viewer\",\n","    \"acl_owner\": \"acl_owner\",\n","    \"authentication_mode\": \"msal_interactive\",\n","    \"authority\": \"\",\n","    \"scopes\": [\"\"],\n","    \"client_id\": \"\",\n","    \"tenant_id\": \"\",\n","    \"redirect_uri\": \"\",\n","    \"access_token_type\" : \"keyvault\",\n","    \"key_vault_name\" : \"\",\n","    \"secret_name\" : \"\",\n","    \"main_table\" : \"main\",\n","    \"logging_table\" : \"logging_info\",\n","    \"run_info_table\" : \"run_info\",\n","    \"delete_existing_table\" : \"no\"\n","}\n","'''\n","\n","# Load the JSON string into a Python dictionary\n","config_dict = json.loads(config_json)\n","\n","# Create a Series from the dictionary\n","config = pd.Series(config_dict)\n","\n","#The number of documents in each batch. If you increase this you could see error messages about the load being too big\n","batch_size = 750\n","\n","display(config)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"c6cb5a07-682e-44f4-a938-4fbe75c95052","normalized_state":"finished","queued_time":"2025-01-30T03:23:04.8130284Z","session_start_time":null,"execution_start_time":"2025-01-30T03:23:04.9616147Z","execution_finish_time":"2025-01-30T03:23:05.7253231Z","parent_msg_id":"eb7212cf-7398-4ab1-8566-dd906f76f36b"},"text/plain":"StatementMeta(, c6cb5a07-682e-44f4-a938-4fbe75c95052, 15, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"server                   https://mahuja.energy.azure.com\ncrs_catalog_url                     /api/crs/catalog/v2/\ncrs_converter_url                 /api/crs/converter/v2/\nentitlements_url                   /api/entitlements/v2/\nfile_url                                   /api/file/v2/\nlegal_url                                 /api/legal/v1/\nschema_url                       /api/schema-service/v1/\nsearch_url                               /api/search/v2/\nstorage_url                             /api/storage/v2/\nunit_url                                   /api/unit/v3/\nworkflow_url                           /api/workflow/v1/\ndata_partition_id                               testdata\nlegal_tag                                      legal_tag\nacl_viewer                                    acl_viewer\nacl_owner                                      acl_owner\nauthentication_mode                     msal_interactive\nauthority                                               \nscopes                                                []\nclient_id                                               \ntenant_id                                               \nredirect_uri                                            \naccess_token_type                               keyvault\nkey_vault_name                                          \nsecret_name                                             \nmain_table                                          main\nlogging_table                               logging_info\nrun_info_table                                  run_info\ndelete_existing_table                                yes\ndtype: object"},"metadata":{}}],"execution_count":13,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"a433877a-c1cf-44c9-a387-acd66d9aff91"},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, MapType\n","\n","# Define the log schema\n","log_schema = StructType([\n","    StructField(\"log_id\", StringType(), False),\n","    StructField(\"log_timestamp\", TimestampType(), False),\n","    StructField(\"log_level\", StringType(), False),\n","    StructField(\"file_name\", StringType(), False),\n","    StructField(\"line_number\", StringType(), False),\n","    StructField(\"message\", StringType(), False)\n","])\n","\n","# Define the data schema\n","schema = StructType([\n","    StructField(\"createTime\", StringType(), True),\n","    StructField(\"kind\", StringType(), True),\n","    StructField(\"authority\", StringType(), True),\n","    StructField(\"namespace\", StringType(), True),\n","    StructField(\"legal\", StringType(), True),\n","    StructField(\"createUser\", StringType(), True),\n","    StructField(\"source\", StringType(), True),\n","    StructField(\"acl\", StringType(), True),\n","    StructField(\"id\", StringType(), True),\n","    StructField(\"type\", StringType(), True),\n","    StructField(\"version\", StringType(), True),\n","    StructField(\"tags\", StringType(), True),\n","    StructField(\"data\", StringType(), True),\n","    StructField(\"modifyUser\", StringType(), True),\n","    StructField(\"modifyTime\", StringType(), True),\n","    StructField(\"ancestry\", StringType(), True),    \n","    StructField(\"ingestTime\", StringType(), True)\n","])\n","\n","run_info_schema = StructType([\n","    StructField(\"run_id\", StringType(), False),\n","    StructField(\"run_timestamp\", LongType(), False)\n","])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"c6cb5a07-682e-44f4-a938-4fbe75c95052","normalized_state":"finished","queued_time":"2025-01-30T03:08:01.7196367Z","session_start_time":null,"execution_start_time":"2025-01-30T03:08:17.3798318Z","execution_finish_time":"2025-01-30T03:08:17.6240987Z","parent_msg_id":"17cda169-e47d-4ba5-9bdc-f08e8991232e"},"text/plain":"StatementMeta(, c6cb5a07-682e-44f4-a938-4fbe75c95052, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"9be7f400-a193-455f-8371-aa070def16fb"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType\n","\n","def create_table(spark: SparkSession, table_name: str, delete_existing_table: bool, schema: StructType) -> None:\n","    # Define the target table name and path\n","    table_path = f\"Tables/{table_name}\"\n","\n","    # Check if the table exists\n","    table_exists = DeltaTable.isDeltaTable(spark, table_path)\n","    print(f\"table exists: {table_exists}\")\n","\n","    if table_exists and delete_existing_table:\n","        # If table exists and we need to delete/overwrite it\n","        deltaTable = DeltaTable.forPath(spark, table_path)\n","        deltaTable.delete()  # This deletes all records in the table, not the table itself\n","        \n","        # Create an empty DataFrame with the schema and overwrite the existing table\n","        empty_df = spark.createDataFrame([], schema)\n","        empty_df.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n","        print(f\"Existing table {table_name} at {table_path} was deleted and recreated with the schema.\")\n","\n","    elif not table_exists:\n","        # If the table does not exist, create it\n","        empty_df = spark.createDataFrame([], schema)\n","        empty_df.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n","        print(f\"Table {table_name} created at {table_path} with the schema.\")\n","    else:\n","        # If the table exists and we should not delete it\n","        print(f\"Table {table_name} already exists at {table_path}. No changes made.\")\n","\n","    return None\n","\n","    from pyspark.sql.utils import AnalysisException\n","\n","def create_table2(spark: SparkSession, table_name: str, delete_existing_table: bool, schema: StructType) -> None:\n","    table_path = f\"Tables/{table_name}\"\n","\n","    try:\n","        table_exists = spark.catalog.tableExists(table_name)\n","        print(f\"table exists: {table_exists}\")\n","    except AnalysisException:\n","        table_exists = False\n","\n","    if table_exists:\n","        if delete_existing_table:\n","            backup_table_name = f\"{table_name}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n","            spark.sql(f\"ALTER TABLE {table_name} RENAME TO {backup_table_name}\")\n","            print(f\"Existing table {table_name} renamed to {backup_table_name} before recreation.\")\n","\n","        # Drop and recreate\n","        spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n","\n","    # Create an empty table with schema\n","    empty_df = spark.createDataFrame([], schema)\n","    empty_df.write.format(\"delta\").saveAsTable(table_name)\n","    print(f\"Table {table_name} created with the schema.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"c6cb5a07-682e-44f4-a938-4fbe75c95052","normalized_state":"finished","queued_time":"2025-01-30T03:22:21.6984252Z","session_start_time":null,"execution_start_time":"2025-01-30T03:22:21.8342413Z","execution_finish_time":"2025-01-30T03:22:22.0634885Z","parent_msg_id":"f041ba1f-0a62-4307-987b-69f3717fbc75"},"text/plain":"StatementMeta(, c6cb5a07-682e-44f4-a938-4fbe75c95052, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"735e320f-de82-4555-8e0f-c74f623fe47f"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n","from delta.tables import DeltaTable\n","from pyspark.sql.functions import current_timestamp, lit\n","from datetime import datetime\n","import uuid\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"LoggingTableCreation\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"c6cb5a07-682e-44f4-a938-4fbe75c95052","normalized_state":"finished","queued_time":"2025-01-30T03:08:01.7208859Z","session_start_time":null,"execution_start_time":"2025-01-30T03:08:18.1519053Z","execution_finish_time":"2025-01-30T03:08:18.4052596Z","parent_msg_id":"6d0941c5-0776-46db-bbf1-27bef3942edb"},"text/plain":"StatementMeta(, c6cb5a07-682e-44f4-a938-4fbe75c95052, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a6d29d5a-4b3b-4a71-81dd-4c8f5478648c"},{"cell_type":"code","source":["main_table = config[\"main_table\"]\n","run_info_table = config[\"run_info_table\"]\n","logging_table = config[\"logging_table\"]\n","\n","delete_existing_table = config.get(\"delete_existing_table\", \"no\").lower() == \"yes\"\n","\n","create_table2(spark, logging_table, delete_existing_table, log_schema)\n","create_table2(spark, run_info_table, delete_existing_table, run_info_schema)\n","create_table2(spark, main_table, delete_existing_table, schema)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"c6cb5a07-682e-44f4-a938-4fbe75c95052","normalized_state":"finished","queued_time":"2025-01-30T03:23:12.6663488Z","session_start_time":null,"execution_start_time":"2025-01-30T03:23:12.7916175Z","execution_finish_time":"2025-01-30T03:23:16.3528961Z","parent_msg_id":"3ebbed04-495e-40ec-9525-9f9ad3857861"},"text/plain":"StatementMeta(, c6cb5a07-682e-44f4-a938-4fbe75c95052, 16, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["table exists: True\nExisting table logging_info renamed to logging_info_backup_20250130_032312 before recreation.\nTable logging_info created with the schema.\n"]}],"execution_count":14,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"3552e107-e18d-4371-89ca-2a2b7c2de326"},{"cell_type":"markdown","source":["# Helper code\n","Below are some cells with code to help developers check imported data and similar QA tasks"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"eb31bce8-db53-49f5-b225-446adda9d529"},{"cell_type":"code","source":["# Code to see imported data - copy this into a cell at the bottom of the batch export to check the number of documents in the lakehouse\n","# Insert the current timestamp\n","run_id = str(uuid.uuid4())\n","run_info_df = spark.createDataFrame([(run_id,)], [\"run_id\"])\n","run_info_df = run_info_df.withColumn(\"run_timestamp\", current_timestamp())\n","\n","run_info_df.write.insertInto(run_info_table, overwrite=False)\n","\n","# Display the 10 documents with the newest createTime\n","df_newest = spark.sql(f\"SELECT * FROM {main_table} ORDER BY createTime DESC LIMIT 10\")\n","display(df_newest)\n","\n","# Query to select all rows from the table\n","df_all = spark.sql(f\"SELECT * FROM {main_table}\")\n","\n","# Count the number of rows\n","num_documents = df_all.count()\n","\n","# Print the number of documents\n","print(f\"Number of documents in {main_table}: {num_documents}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"c6cb5a07-682e-44f4-a938-4fbe75c95052","normalized_state":"finished","queued_time":"2025-01-30T03:08:01.7220506Z","session_start_time":null,"execution_start_time":"2025-01-30T03:08:21.2929197Z","execution_finish_time":"2025-01-30T03:08:42.4368122Z","parent_msg_id":"81331a48-e66a-4011-a281-395fac9413dc"},"text/plain":"StatementMeta(, c6cb5a07-682e-44f4-a938-4fbe75c95052, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"58c340ff-6ec5-4faf-86c1-266d58948eb9","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 58c340ff-6ec5-4faf-86c1-266d58948eb9)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Number of documents in main: 0\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"3ef54abc-dfb3-407d-8011-68238b1341c9"},{"cell_type":"code","source":["# Testcode to see how many documents has been imported and checking for duplicates\n","from pyspark.sql import SparkSession    \n","from pyspark.sql.functions import col, count\n","from delta.tables import DeltaTable\n","\n","# Define the target table name and path\n","target_table_path = f\"Tables/{main_table}\"\n","\n","# Load the Delta table\n","bronze_table = DeltaTable.forPath(spark, target_table_path)\n","\n","# Read the data into a DataFrame\n","df = bronze_table.toDF()\n","\n","# Count the occurrences of each id\n","id_counts = df.groupBy(\"id\").agg(count(\"id\").alias(\"count\"))\n","\n","# Filter for duplicate ids (count > 1)\n","duplicate_ids = id_counts.filter(col(\"count\") > 1)\n","\n","# Show duplicate ids if any\n","if duplicate_ids.count() > 0:\n","    print(\"Duplicate IDs found:\")\n","    duplicate_ids.show(truncate=False)\n","else:\n","    print(\"All IDs are unique.\")\n","\n","# Query to select all rows from the table\n","df_all = spark.sql(f\"SELECT * FROM {main_table}\")\n","\n","# Count the number of rows\n","num_documents = df_all.count()\n","\n","# Print the number of documents\n","print(f\"Number of documents in main: {num_documents}\")\n","\n","\n","# Query to select all rows from the table\n","df_all_run = spark.sql(f\"SELECT * FROM {run_info_table}\")\n","\n","# Count the number of rows\n","num_documents_run = df_all_run.count()\n","\n","# Print the number of documents\n","print(f\"Number of documents in {run_info_table}: {num_documents_run}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"c6cb5a07-682e-44f4-a938-4fbe75c95052","normalized_state":"finished","queued_time":"2025-01-30T03:08:01.722649Z","session_start_time":null,"execution_start_time":"2025-01-30T03:08:42.5721789Z","execution_finish_time":"2025-01-30T03:08:47.5309914Z","parent_msg_id":"64398a43-dc49-4763-8c66-6dca3f326dc6"},"text/plain":"StatementMeta(, c6cb5a07-682e-44f4-a938-4fbe75c95052, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["All IDs are unique.\nNumber of documents in main: 0\nNumber of documents in run_info: 3\n"]}],"execution_count":7,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"8e31aabd-a420-43e9-a58a-e8f23f3ebc10"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"synapse_widget":{"version":"0.1","state":{"58c340ff-6ec5-4faf-86c1-266d58948eb9":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[],"schema":[{"key":"0","name":"createTime","type":"string"},{"key":"1","name":"kind","type":"string"},{"key":"2","name":"authority","type":"string"},{"key":"3","name":"namespace","type":"string"},{"key":"4","name":"legal","type":"string"},{"key":"5","name":"createUser","type":"string"},{"key":"6","name":"source","type":"string"},{"key":"7","name":"acl","type":"string"},{"key":"8","name":"id","type":"string"},{"key":"9","name":"type","type":"string"},{"key":"10","name":"version","type":"string"},{"key":"11","name":"tags","type":"string"},{"key":"12","name":"data","type":"string"},{"key":"13","name":"modifyUser","type":"string"},{"key":"14","name":"modifyTime","type":"string"},{"key":"15","name":"ancestry","type":"string"},{"key":"16","name":"ingestTime","type":"string"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":{"candidateVariableNames":["df_newest"],"dataframeType":"pyspark"}},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1","evaluatesOverAllRecords":false},"viewOptionsGroup":[{"tabItems":[{"type":"table","name":"Table","key":"0","options":{}}]}]}}}}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"140884af-bff0-42b6-9cf3-77db9f7c0f86","known_lakehouses":[{"id":"3412c31e-b682-450b-aa85-48d0c2bfb8ef"},{"id":"a650f518-9bb2-450b-95c0-2425a77baaec"},{"id":"140884af-bff0-42b6-9cf3-77db9f7c0f86"},{"id":"5693c6f8-0b3c-47e4-ad87-10913f01d229"}],"default_lakehouse_name":"adme2","default_lakehouse_workspace_id":"c205926d-e1b8-4068-ba76-36c7f9e21a69"}}},"nbformat":4,"nbformat_minor":5}